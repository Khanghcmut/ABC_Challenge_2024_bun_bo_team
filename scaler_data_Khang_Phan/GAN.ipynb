{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7012296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import pandas as pd # data processing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv1D,Dropout,LSTM, Bidirectional\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "from torch.nn import Transformer\n",
    "from sklearn.preprocessing import StandardScaler #good\n",
    "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler\n",
    "from sklearn import metrics #accuracy measure\n",
    "import joblib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92fcd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=os.listdir(\"./training_set_df_acc/\")\n",
    "df=None\n",
    "for data in training_set:\n",
    "    if data.endswith('.csv'):\n",
    "        file=f\"./training_set_df_acc/{data}\"\n",
    "        if df is None:\n",
    "            df=pd.read_csv(file)\n",
    "        else:\n",
    "            d=pd.read_csv(file)\n",
    "            df=pd.concat([df,d])\n",
    "X=df.drop([\"previous_label\"],axis=1)\n",
    "y=df['label']\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    "# X_train = np.expand_dims(X_train, 2)\n",
    "# X_test = np.expand_dims(X_test, 2)\n",
    "y_train=tf.keras.utils.to_categorical(y_train)\n",
    "y_test=tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a93699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81eade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khangphan/anaconda3/lib/python3.10/site-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "# n_obs, feature, depth = X_train.shape\n",
    "# Mô hình Generator\n",
    "g = Sequential()\n",
    "g.add(Dense(256,  activation=LeakyReLU(alpha=0.2)))\n",
    "g.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "g.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\n",
    "# Vì dữ liệu ảnh MNIST đã chuẩn hóa về [0, 1] nên hàm G khi sinh ảnh ra cũng cần sinh ra ảnh có pixel value trong khoảng [0, 1] => hàm sigmoid được chọn\n",
    "g.add(Dense(2997, activation='sigmoid'))  \n",
    "g.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# Mô hình Discriminator\n",
    "d = Sequential()\n",
    "d.add(Dense(2997, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dense(1024,  activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "d.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "d.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "# Hàm sigmoid cho bài toán binary classification \n",
    "d.add(Dense(9, activation='sigmoid'))\n",
    "d.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "d.trainable = False\n",
    "\n",
    "inputs = Input(shape=(2997,1))\n",
    "hidden = g(inputs)\n",
    "output = d(hidden)\n",
    "gan = Model(inputs, output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc9b0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"D\":[], \"G\":[]}\n",
    "\n",
    "def plot_loss(losses):\n",
    "    d_loss = [v[0] for v in losses[\"D\"]]\n",
    "    g_loss = [v[0] for v in losses[\"G\"]]\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
    "    plt.plot(g_loss, label=\"Generator loss\")\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e344056",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim=1\n",
    "gloss=[]\n",
    "dloss=[]\n",
    "def train(epochs=1, plt_frq=1, BATCH_SIZE=2997):\n",
    "    # Tính số lần chạy trong mỗi epoch\n",
    "    batchCount = int(X_train.shape[0] / BATCH_SIZE)\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', BATCH_SIZE)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "    \n",
    "    for e in tqdm_notebook(range(1, epochs+1)):\n",
    "        if e == 1 or e%plt_frq == 0:\n",
    "            print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in range(batchCount):\n",
    "            # Lấy ngẫu nhiên các ảnh từ MNIST dataset (ảnh thật)\n",
    "            image_batch = X_train[np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)]\n",
    "            # Sinh ra noise ngẫu nhiên\n",
    "            print(image_batch)\n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
    "            \n",
    "            # Dùng Generator sinh ra ảnh từ noise\n",
    "            generated_images = g.predict(noise)\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            # Tạo label\n",
    "            y = np.zeros(2*BATCH_SIZE)\n",
    "            y[:BATCH_SIZE] = 0.9  # gán label bằng 0.9 cho những ảnh từ MNIST dataset và 0 cho ảnh sinh ra bởi Generator\n",
    "\n",
    "            # Train discriminator\n",
    "            d.trainable = True\n",
    "            d_loss = d.fit(X, y)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
    "            # Khi train Generator gán label bằng 1 cho những ảnh sinh ra bởi Generator -> cố gắng lừa Discriminator. \n",
    "            y2 = np.ones(BATCH_SIZE)\n",
    "            # Khi train Generator thì không cập nhật hệ số của Discriminator.\n",
    "            d.trainable = False\n",
    "            g_loss = gan.fit(noise, y2)\n",
    "            print(d_loss)\n",
    "            print(g_loss)\n",
    "#         # Lưu loss function\n",
    "            losses[\"D\"].append(d_loss)\n",
    "            losses[\"G\"].append(g_loss)\n",
    "\n",
    "        # Vẽ các số được sinh ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba30e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0b73eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_70267/1510339043.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm_notebook(range(1, epochs+1)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 200\n",
      "Batch size: 2997\n",
      "Batches per epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0983a34cd38644dab9b919957ac24928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n",
      "--------------- Epoch 20 ---------------\n",
      "--------------- Epoch 40 ---------------\n",
      "--------------- Epoch 60 ---------------\n",
      "--------------- Epoch 80 ---------------\n",
      "--------------- Epoch 100 ---------------\n",
      "--------------- Epoch 120 ---------------\n",
      "--------------- Epoch 140 ---------------\n",
      "--------------- Epoch 160 ---------------\n",
      "--------------- Epoch 180 ---------------\n",
      "--------------- Epoch 200 ---------------\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "train(epochs=200, plt_frq=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1850b210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': [], 'G': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024685fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 163s 2s/step\n"
     ]
    }
   ],
   "source": [
    "generated_images = g.predict(X_train)\n",
    "generated_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
