{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9511b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import pandas as pd # data processing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv1D,Dropout,LSTM, Bidirectional\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import torch\n",
    "from torch.nn import Transformer\n",
    "from sklearn.preprocessing import StandardScaler #bad\n",
    "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6bbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"./\"\n",
    "USER_ID = \"N01T1\"\n",
    "keypoint_csv = DATA_FOLDER + \"train/{}_keypoint.csv\".format(USER_ID)\n",
    "ann_csv = DATA_FOLDER + \"/ann/{}_ann.csv\".format(USER_ID)\n",
    "standartscaler=MaxAbsScaler()\n",
    "\n",
    "TEST_ID = \"N02T1\"\n",
    "test_keypoint_csv = DATA_FOLDER + \"/train/{}_keypoint.csv\".format(TEST_ID)\n",
    "test_ann_csv = DATA_FOLDER + \"/ann/{}_ann.csv\".format(TEST_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05ea237",
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = 30 # sampling rate\n",
    "TOTAL_CLASSESS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e80f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(keypoint_csv, ann_csv):\n",
    "    kp_df = pd.read_csv(keypoint_csv)\n",
    "    kp_df = kp_df.loc[:, ~kp_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    ann_df = pd.read_csv(ann_csv)\n",
    "    ann_df = ann_df.loc[:, ~ann_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    return kp_df, ann_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac38bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_df, ann_df = load_data(keypoint_csv, ann_csv)\n",
    "\n",
    "test_kp_df, test_ann_df = load_data(test_keypoint_csv, test_ann_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71837948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['right_ankle_y', 'right_ankle_conf'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65d4ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nose_x</th>\n",
       "      <th>left_eye_x</th>\n",
       "      <th>right_eye_x</th>\n",
       "      <th>left_ear_x</th>\n",
       "      <th>right_ear_x</th>\n",
       "      <th>left_shoulder_x</th>\n",
       "      <th>right_shoulder_x</th>\n",
       "      <th>left_elbow_x</th>\n",
       "      <th>right_elbow_x</th>\n",
       "      <th>left_wrist_x</th>\n",
       "      <th>right_wrist_x</th>\n",
       "      <th>left_hip_x</th>\n",
       "      <th>right_hip_x</th>\n",
       "      <th>left_knee_x</th>\n",
       "      <th>right_knee_x</th>\n",
       "      <th>left_ankle_x</th>\n",
       "      <th>right_ankle_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>363.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>412.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.50</td>\n",
       "      <td>22.1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>361.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>399.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.00</td>\n",
       "      <td>21.3750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>364.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>414.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.50</td>\n",
       "      <td>20.3125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>410.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.50</td>\n",
       "      <td>20.8125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>366.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>413.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>20.3125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6985</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>452.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>308.50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>372.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>420.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>322.50</td>\n",
       "      <td>75.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>452.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>448.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>297.50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6990 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nose_x  left_eye_x  right_eye_x  left_ear_x  right_ear_x  \\\n",
       "0        0.0         0.0          0.0         0.0          0.0   \n",
       "1        0.0         0.0          0.0         0.0          0.0   \n",
       "2        0.0         0.0          0.0         0.0          0.0   \n",
       "3        0.0         0.0          0.0         0.0          0.0   \n",
       "4        0.0         0.0          0.0         0.0          0.0   \n",
       "...      ...         ...          ...         ...          ...   \n",
       "6985     0.0         0.0          0.0         0.0          0.0   \n",
       "6986     0.0         0.0          0.0         0.0          0.0   \n",
       "6987     0.0         0.0          0.0         0.0          0.0   \n",
       "6988     0.0         0.0          0.0         0.0          0.0   \n",
       "6989     0.0         0.0          0.0         0.0          0.0   \n",
       "\n",
       "      left_shoulder_x  right_shoulder_x  left_elbow_x  right_elbow_x  \\\n",
       "0                0.00               0.0        363.50            0.0   \n",
       "1                0.00               0.0        361.50            0.0   \n",
       "2                0.00               0.0        364.50            0.0   \n",
       "3                0.00               0.0        365.00            0.0   \n",
       "4                0.00               0.0        366.25            0.0   \n",
       "...               ...               ...           ...            ...   \n",
       "6985             0.00               0.0        378.00            0.0   \n",
       "6986           330.25               0.0        372.75            0.0   \n",
       "6987             0.00               0.0        373.00            0.0   \n",
       "6988             0.00               0.0        380.00            0.0   \n",
       "6989             0.00               0.0        373.50            0.0   \n",
       "\n",
       "      left_wrist_x  right_wrist_x  left_hip_x  right_hip_x  left_knee_x  \\\n",
       "0           412.50            0.0      279.50      22.1250          0.0   \n",
       "1           399.50            0.0      282.00      21.3750          0.0   \n",
       "2           414.00            0.0      279.50      20.3125          0.0   \n",
       "3           410.75            0.0      279.50      20.8125          0.0   \n",
       "4           413.25            0.0      280.00      20.3125          0.0   \n",
       "...            ...            ...         ...          ...          ...   \n",
       "6985        452.25            0.0      308.50       0.0000          0.0   \n",
       "6986        420.75            0.0      322.50      75.0000          0.0   \n",
       "6987        452.50            0.0      304.75       0.0000          0.0   \n",
       "6988        461.75            0.0      303.00       0.0000          0.0   \n",
       "6989        448.25            0.0      297.50       0.0000          0.0   \n",
       "\n",
       "      right_knee_x  left_ankle_x  right_ankle_x  \n",
       "0              0.0           0.0            0.0  \n",
       "1              0.0           0.0            0.0  \n",
       "2              0.0           0.0            0.0  \n",
       "3              0.0           0.0            0.0  \n",
       "4              0.0           0.0            0.0  \n",
       "...            ...           ...            ...  \n",
       "6985           0.0           0.0            0.0  \n",
       "6986           0.0           0.0            0.0  \n",
       "6987           0.0           0.0            0.0  \n",
       "6988           0.0           0.0            0.0  \n",
       "6989           0.0           0.0            0.0  \n",
       "\n",
       "[6990 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp_df.loc[:,kp_df.columns.str.contains(\"_x\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97dd491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb63bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa2611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_position = \"left_wrist\"  # choose keypoint you want to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_DICT = {\"nose\": 0,\n",
    "                  \"left_eye\": 1,\n",
    "                  \"right_eye\": 2,\n",
    "                  \"left_ear\": 3,\n",
    "                  \"right_ear\": 4,\n",
    "                  \"left_shoulder\": 5,\n",
    "                  \"right_shoulder\": 6,\n",
    "                  \"left_elbow\": 7,\n",
    "                  \"right_elbow\": 8,\n",
    "                  \"left_wrist\": 9,\n",
    "                  \"right_wrist\": 10,\n",
    "                  \"left_hip\": 11,\n",
    "                  \"right_hip\": 12,\n",
    "                  \"left_knee\": 13,\n",
    "                  \"right_knee\": 14,\n",
    "                  \"left_ankle\": 15,\n",
    "                  \"right_ankle\": 16}\n",
    "\n",
    "values = np.array(kp_df)[:, int(KEYPOINTS_DICT[keypoint_position]*3):int(KEYPOINTS_DICT[keypoint_position]*3 + 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db495856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(30,15))\n",
    "fig, axs = plt.subplots(3, 1, figsize=(30,15),  layout=\"constrained\")\n",
    "cols_name = [\"x\", \"y\", \"conf\"]\n",
    "for x in range(3):\n",
    "    axs[x].plot(values[:, x])\n",
    "    axs[x].set_title(cols_name[x], y=0, loc='right', fontsize=20)\n",
    "    axs[x].set_xlabel(\"Time (second)\")\n",
    "    axs[x].set_ylabel(cols_name[x])\n",
    "\n",
    "x_ticks = np.arange(0, len(values), 10*FS)\n",
    "x_ticklabels = np.arange(0, len(x_ticks))*10\n",
    "plt.setp(axs, xticks=x_ticks, xticklabels=x_ticklabels)\n",
    "fig.suptitle('{} - {}'.format(USER_ID, keypoint_position), size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1456c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data_count = ann_df['annotation_str'].value_counts()\n",
    "tmp_data_count.plot(kind='bar')\n",
    "plt.title('Data Amount of Each Activity Type')\n",
    "plt.xlabel('Activity Type ID')\n",
    "plt.ylabel('Data Amount [number of samples]')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del tmp_data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da076544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_kp(kp_df):\n",
    "    kp_df = kp_df.loc[:, ~kp_df.columns.str.contains(\n",
    "    'conf|left_knee|right_knee|left_ankle|right_ankle', regex=True)]\n",
    "    return kp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_df = remove_redundant_kp(kp_df)\n",
    "\n",
    "test_kp_df = remove_redundant_kp(test_kp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c557253",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTH_LEN = 3 # seconds\n",
    "\n",
    "def smooth_kp(kp_col):\n",
    "    zero_idx = np.flatnonzero(kp_col == 0)\n",
    "    split_idx = np.split(zero_idx, np.flatnonzero(np.diff(zero_idx) > 1) + 1)\n",
    "    for each_split_idx in split_idx:\n",
    "        if len(each_split_idx) == 0 or each_split_idx[0] == 0 or each_split_idx[-1] == (len(kp_col) - 1) or len(each_split_idx) > SMOOTH_LEN*FS:\n",
    "            continue\n",
    "        xp = [each_split_idx[0] - 1, each_split_idx[-1] + 1]\n",
    "        fp = kp_col[xp]\n",
    "        interp_kp = np.interp(each_split_idx, xp, fp)\n",
    "        kp_col[each_split_idx] = interp_kp\n",
    "    return kp_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features=[ 'right_shoulder_x','right_shoulder_y', 'right_shoulder nose_ dx', 'right_shoulder nose dy',\n",
    "       'right_shoulder left_eye_ dx', 'right_shoulder left_eye dy',\n",
    "       'right_shoulder right_eye_ dx', 'right_shoulder right_eye dy',\n",
    "       'right_shoulder left_ear_ dx', 'right_shoulder left_ear dy',\n",
    "       'right_shoulder right_ear_ dx', 'right_shoulder right_ear dy',\n",
    "       'right_shoulder left_shoulder_ dx', 'right_shoulder left_shoulder dy',\n",
    "       'right_shoulder left_elbow_ dx', 'right_shoulder left_elbow dy',\n",
    "       'right_shoulder right_elbow_ dx', 'right_shoulder right_elbow dy',\n",
    "       'right_shoulder left_wrist_ dx', 'right_shoulder left_wrist dy',\n",
    "       'right_shoulder right_wrist_ dx', 'right_shoulder right_wrist dy',\n",
    "       'right_shoulder left_hip_ dx', 'right_shoulder left_hip dy',\n",
    "       'right_shoulder right_hip_ dx', 'right_shoulder right_hip dy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [e for e in kp_df.columns if e not in ('right_shoulder_y','right_shoulder_x')]:\n",
    "    if i[-1]=='y':\n",
    "        feat_name=\"right_shoulder \"+i[:-2]+' dy'\n",
    "        kp_df[feat_name]=kp_df['right_shoulder_y']-kp_df[i]\n",
    "    else:\n",
    "        feat_name=\"right_shoulder \"+i[:-1]+' dx'\n",
    "        kp_df[feat_name]=kp_df['right_shoulder_x']-kp_df[i]\n",
    "# kp_df=kp_df[used_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e917c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [e for e in test_kp_df.columns if e not in ('right_shoulder_y','right_shoulder_x')]:\n",
    "    if i[-1]=='y':\n",
    "        feat_name=\"right_shoulder \"+i[:-2]+' dy'\n",
    "        test_kp_df[feat_name]=test_kp_df['right_shoulder_y']-test_kp_df[i]\n",
    "    else:\n",
    "        feat_name=\"right_shoulder \"+i[:-1]+' dx'\n",
    "        test_kp_df[feat_name]=test_kp_df['right_shoulder_x']-test_kp_df[i]\n",
    "\n",
    "# test_kp_df=test_kp_df[used_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd58c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kp_df=standartscaler.fit_transform(kp_df)\n",
    "# test_kp_df=standartscaler.fit_transform(test_kp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kp_df=pd.DataFrame(kp_df)\n",
    "# test_kp_df=pd.DataFrame(test_kp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5407ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_angle(a, b, c):\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    return angle\n",
    "\n",
    "\n",
    "def extract_joint_angles(kp_data, steps=2):\n",
    "    # steps = 2 if kp_data is removed conf columns\n",
    "    # steps = 3 if kp_data has conf columns\n",
    "    left_elbow_shoulder_hip = np.asarray([cal_angle(kp_data[i, 7*steps:(7*steps+2)], kp_data[i, 5*steps:(5*steps+2)], kp_data[i, 11*steps:(11*steps+2)])\n",
    "                                          for i in range(len(kp_data))])\n",
    "    left_elbow_shoulder_hip = np.nan_to_num(left_elbow_shoulder_hip)\n",
    "\n",
    "\n",
    "    right_elbow_shoulder_hip = np.asarray([cal_angle(kp_data[i, 8*steps:(8*steps+2)], kp_data[i, 6*steps:(6*steps+2)], kp_data[i, 12*steps:(12*steps+2)])\n",
    "                                            for i in range(len(kp_data))])\n",
    "    right_elbow_shoulder_hip = np.nan_to_num(right_elbow_shoulder_hip)\n",
    "\n",
    "\n",
    "    left_wrist_elbow_shoulder = np.asarray([cal_angle(kp_data[i, 9*steps:(9*steps+2)], kp_data[i, 7*steps:(7*steps+2)], kp_data[i, 5*steps:(5*steps + 2)])\n",
    "                                            for i in range(len(kp_data))])\n",
    "    left_wrist_elbow_shoulder = np.nan_to_num(left_wrist_elbow_shoulder)\n",
    "\n",
    "\n",
    "    right_wrist_elbow_shoulder = np.asarray([cal_angle(kp_data[i, 10*steps:(10*steps+2)], kp_data[i, 8*steps:(8*steps+2)], kp_data[i, 6*steps:(6*steps+2)])\n",
    "                                              for i in range(len(kp_data))])\n",
    "    right_wrist_elbow_shoulder = np.nan_to_num(right_wrist_elbow_shoulder)\n",
    "\n",
    "\n",
    "    right_elbow_shoulder = np.asarray([cal_angle(kp_data[i, 8*steps:(8*steps+2)], kp_data[i, 6*steps:(6*steps+2)], kp_data[i, 5*steps:(5*steps+2)])\n",
    "                                              for i in range(len(kp_data))])\n",
    "    right_elbow_shoulder = np.nan_to_num(right_elbow_shoulder)\n",
    "\n",
    "\n",
    "    left_elbow_shoulder = np.asarray([cal_angle(kp_data[i, 6*steps:(6*steps+2)], kp_data[i, 5*steps:(5*steps+2)], kp_data[i, 7*steps:(7*steps+2)])\n",
    "                                              for i in range(len(kp_data))])\n",
    "    left_elbow_shoulder = np.nan_to_num(left_elbow_shoulder)\n",
    "\n",
    "\n",
    "    joint_angles = np.array([left_elbow_shoulder_hip,\n",
    "                    right_elbow_shoulder_hip, left_wrist_elbow_shoulder, right_wrist_elbow_shoulder, right_elbow_shoulder, left_elbow_shoulder]).T\n",
    "\n",
    "    return joint_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_velocity(kp_data):\n",
    "    velocity = np.diff(kp_data, axis=0)\n",
    "    return velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f34d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49558e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b389a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(kp_df.columns)):\n",
    "    kp_df.iloc[:, i] = smooth_kp(np.array(kp_df.iloc[:, i]))\n",
    "    test_kp_df.iloc[:, i] = smooth_kp(np.array(test_kp_df.iloc[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_position = \"left_elbow\"  # choose keypoint you want to visualize\n",
    "values = np.array(kp_df)[:, int(KEYPOINTS_DICT[keypoint_position]*2):int(KEYPOINTS_DICT[keypoint_position]*2 + 2)]\n",
    "\n",
    "fig = plt.figure(figsize=(30,15))\n",
    "fig, axs = plt.subplots(2, 1, figsize=(30,15),  layout=\"constrained\")\n",
    "cols_name = [\"x\", \"y\"]\n",
    "for x in range(2):\n",
    "    axs[x].plot(values[:, x])\n",
    "    axs[x].set_title(cols_name[x], y=0, loc='right', fontsize=20)\n",
    "    axs[x].set_xlabel(\"Time (second)\")\n",
    "    axs[x].set_ylabel(cols_name[x])\n",
    "\n",
    "x_ticks = np.arange(0, len(values), 10*FS)\n",
    "x_ticklabels = np.arange(0, len(x_ticks))*10\n",
    "plt.setp(axs, xticks=x_ticks, xticklabels=x_ticklabels)\n",
    "fig.suptitle('{} - {}'.format(USER_ID, keypoint_position), size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4dec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(data, max_time, sub_window_size, stride_size):\n",
    "    sub_windows = np.arange(sub_window_size)[None, :] + np.arange(0, max_time, stride_size)[:, None]\n",
    "\n",
    "    row, col = np.where(sub_windows >= max_time)\n",
    "    uniq_row = len(np.unique(row))\n",
    "\n",
    "    if uniq_row > 0 and row[0] > 0:\n",
    "        sub_windows = sub_windows[:-uniq_row, :]\n",
    "\n",
    "    return data[sub_windows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d965ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def extract_feature(data, fs):\n",
    "    mean_ft = np.mean(data, axis=0)\n",
    "    std_ft = np.std(data, axis=0)\n",
    "    max_ft = np.max(data, axis=0)\n",
    "    min_ft = np.min(data, axis=0)\n",
    "    var_ft = np.var(data, axis=0)\n",
    "    med_ft = np.median(data, axis=0)\n",
    "    sum_ft = np.sum(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    kurtosis=scipy.stats.kurtosis(data)\n",
    "    skew=scipy.stats.skew(data)\n",
    "    features = np.array([mean_ft, std_ft, max_ft, min_ft, var_ft, med_ft, sum_ft,std,kurtosis,skew]).T.flatten()\n",
    "    features = np.nan_to_num(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2 # seconds\n",
    "OVERLAP_RATE = 0.5 * WINDOW_SIZE # overlap 50% of window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec300b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing training data\n",
    "all_feature = []\n",
    "all_label = []\n",
    "\n",
    "for i in range(len(ann_df)):\n",
    "    seg = kp_df.loc[int(ann_df['start_time'][i]*FS): int(ann_df['stop_time'][i]*FS)]\n",
    "    seg_label = ann_df[\"annotation\"].iloc[i]\n",
    "    if len(seg) > 0 and (len(seg) >= WINDOW_SIZE * FS):\n",
    "        # Calculate joint angles from keypoint data\n",
    "        joint_angles = extract_joint_angles(np.array(seg))\n",
    "\n",
    "        # Segment keypoint data and joint angles by WINDOW_SIZE and OVERLAP_RATE\n",
    "        ws_seg = segment(np.array(seg), max_time=len(seg), sub_window_size=WINDOW_SIZE * FS, stride_size=int((WINDOW_SIZE - OVERLAP_RATE) * FS))\n",
    "        joint_angles_seg = segment(joint_angles, max_time=len(seg), sub_window_size=WINDOW_SIZE * FS,\n",
    "                                        stride_size=int((WINDOW_SIZE - OVERLAP_RATE) * FS))\n",
    "\n",
    "        # Calculate velocity from each segment of keypoint data\n",
    "        velocity_seg = [extract_velocity(ws_seg[i]) for i in range(len(ws_seg))]\n",
    "\n",
    "        # Calculate features from each segment of keypoint data, joint angles and velocity\n",
    "        feature_seg = [extract_feature(ws_seg[i], FS) for i in range(len(ws_seg))]\n",
    "        feature_joint_angles_seg = [extract_feature(joint_angles_seg[i], FS) for i in\n",
    "                                  range(len(joint_angles_seg))]\n",
    "        feature_velocity_seg = [extract_feature(extract_velocity(ws_seg[i]), FS) for i in range(len(ws_seg))]\n",
    "\n",
    "        # Concatenate all features\n",
    "        feature_seg = np.concatenate([feature_seg, feature_joint_angles_seg, feature_velocity_seg], axis=1)\n",
    "\n",
    "        all_feature.extend(feature_seg)\n",
    "        all_label.extend([int(seg_label)]*len(ws_seg))\n",
    "print(len(feature_seg[0])) \n",
    "print(len(feature_joint_angles_seg[0]))\n",
    "print(feature_seg[0])\n",
    "\n",
    "all_feature = np.array(all_feature)\n",
    "\n",
    "all_label = np.array(all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3359d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a254a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing test data\n",
    "test_all_feature = []\n",
    "test_all_label = []\n",
    "\n",
    "for i in range(len(test_ann_df)):\n",
    "    seg = test_kp_df.loc[int(test_ann_df['start_time'][i]*FS): int(test_ann_df['stop_time'][i]*FS)]\n",
    "    seg_label = test_ann_df[\"annotation\"].iloc[i]\n",
    "    if len(seg) > 0 and (len(seg) >= WINDOW_SIZE * FS):\n",
    "        # Calculate joint angles from keypoint data\n",
    "        joint_angles = extract_joint_angles(np.array(seg))\n",
    "\n",
    "        # Segment keypoint data and joint angles by WINDOW_SIZE and OVERLAP_RATE\n",
    "        ws_seg = segment(np.array(seg), max_time=len(seg), sub_window_size=WINDOW_SIZE * FS, stride_size=int((WINDOW_SIZE - OVERLAP_RATE) * FS))\n",
    "        joint_angles_seg = segment(joint_angles, max_time=len(seg), sub_window_size=WINDOW_SIZE * FS,\n",
    "                                        stride_size=int((WINDOW_SIZE - OVERLAP_RATE) * FS))\n",
    "\n",
    "        # Calculate velocity from each segment of keypoint data\n",
    "        velocity_seg = [extract_velocity(ws_seg[i]) for i in range(len(ws_seg))]\n",
    "\n",
    "        # Calculate features from each segment of keypoint data, joint angles and velocity\n",
    "        feature_seg = [extract_feature(ws_seg[i], FS) for i in range(len(ws_seg))]\n",
    "        feature_joint_angles_seg = [extract_feature(joint_angles_seg[i], FS) for i in\n",
    "                                  range(len(joint_angles_seg))]\n",
    "        feature_velocity_seg = [extract_feature(extract_velocity(ws_seg[i]), FS) for i in range(len(ws_seg))]\n",
    "\n",
    "        # Concatenate all features\n",
    "        feature_seg = np.concatenate([feature_seg, feature_joint_angles_seg, feature_velocity_seg], axis=1)\n",
    "\n",
    "        test_all_feature.extend(feature_seg)\n",
    "        test_all_label.extend([int(seg_label)]*len(ws_seg))\n",
    "        \n",
    "test_all_feature = np.array(test_all_feature)\n",
    "\n",
    "test_all_label = np.array(test_all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential()\n",
    "model_rnn.add(LSTM(units=64, return_sequences=True, input_shape=(len(all_feature[1]), 1)))\n",
    "model_rnn.add(Conv1D(128, 6))\n",
    "model_rnn.add(LSTM(units = 64, return_sequences = True))\n",
    "model_rnn.add(Dropout(0.2))\n",
    "model_rnn.add(Conv1D(128, 3))\n",
    "model_rnn.add(LSTM(units = 64, return_sequences = True))\n",
    "\n",
    "model_rnn.add(Dropout(0.2))\n",
    "model_rnn.add(LSTM(units = 64))\n",
    "\n",
    "model_rnn.add(Dense(units = 128))\n",
    "model_rnn.add(Dense(units = 64))\n",
    "\n",
    "model_rnn.add(Dense(units = 9, activation=\"softmax\")) #4 as the output classes\n",
    "\n",
    "# Define the BRNN model with LSTM layers\n",
    "model_brnn = keras.Sequential([\n",
    "    layers.Bidirectional(layers.LSTM(64, activation='relu', return_sequences=True), \n",
    "                         input_shape=(len(all_feature[1]), 1)),\n",
    "#     layers.Bidirectional(layers.LSTM(64, activation='relu')),\n",
    "#     layers.Bidirectional(layers.LSTM(32, activation='relu')),\n",
    "    layers.Bidirectional(layers.LSTM(16, activation='relu')),\n",
    "    \n",
    "    layers.Dense(9, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e94c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Compile the RNN model\n",
    "model_rnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compile the BRNN model\n",
    "model_brnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the RNN model with early stopping\n",
    "\n",
    "## model_rnn.fit(all_feature, all_label, epochs=100, batch_size=500, validation_data=(X_vad, y_vad), callbacks=[early_stopping])\n",
    "\n",
    "# Train the BRNN model with early stopping\n",
    "\n",
    "model_brnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history_brnn=model_brnn.fit(all_feature, all_label, epochs=10, batch_size=500,\n",
    "                            validation_data=(test_all_feature, test_all_label), callbacks=[early_stopping])\n",
    "# , callbacks=[early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7dd688",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_brnn.history['accuracy'], label='accuracy')\n",
    "plt.plot(history_brnn.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "plt.plot(history_brnn.history['loss'], label='loss')\n",
    "# plt.plot(history_brnn.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ad7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier#XGBcalssifier\n",
    "xgbcl=XGBClassifier( eval_metric='logloss', random_state=10)\n",
    "xgbcl.fit(all_feature, all_label)\n",
    "pred=xgbcl.predict(test_all_feature)\n",
    "from sklearn import metrics #accuracy measure\n",
    "print(metrics.classification_report(pred,test_all_label))\n",
    "ax,fig=plt.subplots(figsize=(10,10))\n",
    "conf=metrics.confusion_matrix(pred,test_all_label, normalize='true')\n",
    "sns.heatmap(conf,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b63599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f340faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "rf=RandomForestClassifier(random_state=10)\n",
    "rf.fit(all_feature, all_label)\n",
    "pred=rf.predict(test_all_feature)\n",
    "\n",
    "print(metrics.classification_report(pred,test_all_label))\n",
    "ax,fig=plt.subplots(figsize=(10,10))\n",
    "conf=metrics.confusion_matrix(pred,test_all_label, normalize='true')\n",
    "sns.heatmap(conf,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a40783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "# try:\n",
    "# oversampling the train dataset using SMOTE\n",
    "sm = RandomOverSampler()\n",
    "all_feature_smote, all_label_smote = sm.fit_resample(all_feature, all_label)\n",
    "rf=RandomForestClassifier(random_state=10)\n",
    "rf.fit(all_feature_smote, all_label_smote)\n",
    "pred=rf.predict(test_all_feature)\n",
    "\n",
    "print(metrics.classification_report(pred,test_all_label))\n",
    "ax,fig=plt.subplots(figsize=(10,10))\n",
    "conf=metrics.confusion_matrix(pred,test_all_label, normalize='true')\n",
    "sns.heatmap(conf,annot=True)\n",
    "# except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864429c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'a':all_label})\n",
    "df['a'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_label.shape\n",
    "test_all_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hmmlearn import hmm\n",
    "# hidden_markov = hmm.GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=100)\n",
    "# hidden_markov.fit(all_feature)\n",
    "# pred=hidden_markov.predict(all_feature)\n",
    "# print(metrics.classification_report(pred,test_all_label))\n",
    "# ax,fig=plt.subplots(figsize=(10,10))\n",
    "# conf=metrics.confusion_matrix(pred,test_all_label, normalize='true')\n",
    "# sns.heatmap(conf,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37682ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(32, 32, activation='relu', input_shape=(len(all_feature[1]), 1)))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Conv1D(64, 64, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Conv1D(64, 32, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='softmax'))\n",
    "# model.add(layers.Dense(32, activation='softmax'))\n",
    "model.add(layers.Dense(9,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(all_feature, all_label, epochs=10, \n",
    "                    validation_data=(test_all_feature,test_all_label), \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa64cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test_all_feature)\n",
    "pred=np.argmax(pred,axis=1)\n",
    "print(metrics.classification_report(pred,test_all_label))\n",
    "ax,fig=plt.subplots(figsize=(10,10))\n",
    "conf=metrics.confusion_matrix(pred,test_all_label, normalize='true')\n",
    "sns.heatmap(conf,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b939477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation , Dropout, BatchNormalization, AveragePooling1D, ZeroPadding1D\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "X_train = np.expand_dims(all_feature, 2)\n",
    "X_test = np.expand_dims(test_all_feature, 2)\n",
    "y_train=tf.keras.utils.to_categorical(all_label)\n",
    "y_test=tf.keras.utils.to_categorical(test_all_label)\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv1D(filters=F1, kernel_size=1, strides=1, padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=2, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv1D(filters=F2, kernel_size=f, strides=1, padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=2, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv1D(filters=F3, kernel_size=1, strides=1, padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=2, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])# SKIP Connection\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "def convolutional_block(X, f, filters, stage, block, s=2):   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv1D(filters=F1, kernel_size=1, strides=s, padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=2, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv1D(filters=F2, kernel_size=f, strides=1, padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=2, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv1D(filters=F3, kernel_size=1, strides=1, padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=2, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    X_shortcut = Conv1D(filters=F3, kernel_size=1, strides=s, padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=2, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "n_obs, feature, depth = X_train.shape\n",
    "# feature,\n",
    "def ResNet(input_shape=(feature,depth)):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = ZeroPadding1D(2)(X_input)\n",
    "    X = Conv1D(128,128 , name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=2, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling1D(3, strides= 2)(X)\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [128, 128, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 256], stage=2, block='c')\n",
    "\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    X = AveragePooling1D(pool_size= 2, padding='same')(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet')\n",
    "    return model\n",
    "base_model = ResNet(input_shape=(feature,1))\n",
    "headModel = base_model.output\n",
    "headModel = Flatten()(headModel)\n",
    "headModel=Dense(256, activation='relu', name='fc1',kernel_initializer=glorot_uniform(seed=0))(headModel)\n",
    "headModel=Dense(128, activation='relu', name='fc2',kernel_initializer=glorot_uniform(seed=0))(headModel)\n",
    "headModel = Dense(9,activation='softmax', name='fc3',kernel_initializer=glorot_uniform(seed=0))(headModel)\n",
    "model = Model(inputs=base_model.input, outputs=headModel)\n",
    "model.compile(\n",
    "          loss = 'categorical_crossentropy',\n",
    "          optimizer = 'Adam',metrics = ['AUC','accuracy']\n",
    "                )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(\n",
    "      X_train,y_train,validation_data=(X_test,y_test),\n",
    "                              epochs=50,batch_size=512,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751c898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ad07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ebc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c85a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114276a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae0bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd1081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5d501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47ee39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87381d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c292c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5877f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a39b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8fe43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338fa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796222a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b40c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9e6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09fa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681767b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7dbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
